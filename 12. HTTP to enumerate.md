# HTTP 

* If we run whatweb on the target IP and it returns: `X-XSS-Protection[0]` --> it means There's no cross-site scripting protection.
* Another tool to get more information: `http [IP]`
* To view the closest rendered image of the website: `browsh http://[IP]`

## DIRB wordlist
* The term "DIRB" often refers to a popular web directory and file brute-forcing tool used by penetration testers and security professionals to discover hidden files and directories on a web server.
* DIRB stands for "Directory Buster." In the context of DIRB, a wordlist is a list of potential directory and file names that the tool uses to perform brute-force attacks on a web server's directory structure.
* A wordlist typically contains a large number of common directory and file names that might exist on a web server.
* DIRB takes each item from the wordlist and sends HTTP requests to the target server to check if those directories or files are accessible.
* By doing so, it helps in identifying hidden or sensitive resources that are not explicitly linked from the website's visible pages.
* `dirb [url]` ---> noot just the IP.

## HTTP IIS nmap scripts
* `nmap -p 80 --script http-enum [IP]`
* `nmap -p 80 --script http-headers [IP]`
* `nmap -p 80 --script http-methods --script-args http-methods.url-path=/webdav/[IP]`
  * The command you've provided is an Nmap command used for network scanning and, in this case, for performing a specific HTTP-related script scan on the target host, which is specified as "10.0.0.1." Let's break down the command step by step:

  * nmap: This is the command to run Nmap, a powerful open-source network scanning tool used for discovering devices running on a network and identifying open ports, services, and potential vulnerabilities.

  * -p 80: This option specifies that Nmap should scan port 80 on the target host. Port 80 is the default port for HTTP traffic, so this scan is focused on the web-related services running on the target.

  * --script http-methods: This option tells Nmap to use the "http-methods" script. The "http-methods" script is part of Nmap's scripting engine and is used to identify which HTTP methods (e.g., GET, POST, PUT, DELETE) are supported by a web server.

  * --script-args http-methods.url-path=/webdav/: Here, you are passing an argument to the "http-methods" script. You specify the URL path as "/webdav/." This means that Nmap will perform the HTTP methods scan specifically on the "/webdav/" path of the web server, checking which HTTP methods are allowed for that path.

  * 10.0.0.1: This is the IP address of the target host you want to scan. Replace this with the actual IP address you want to scan.
  * This can be useful for understanding the security posture of a web server, particularly if it's running WebDAV (Web-based Distributed Authoring and Versioning) services, which is often used for collaborative file management over HTTP. 

  * In summary, the Nmap command you've provided is instructing Nmap to scan port 80 on the target host (presumably a web server) and use the "http-methods" script to determine which HTTP methods are supported for the "/webdav/" path on that server. This can be useful for understanding the security posture of a web server, particularly if it's running WebDAV (Web-based Distributed Authoring and Versioning) services, which is often used for collaborative file management over HTTP.
  * The scan helps identify which HTTP methods are permitted, potentially revealing vulnerabilities or misconfigurations in the server.
 
# HTTP Apache recon
* `nmap -p 80 -sV -script banner [IP]`  --> this will tell which version of Apche is running on the HTTP port 80
* `use auxiliary/scanner/http/http_version`
* **curl command**
  *   Running the command curl 10.0.0.1 will attempt to make an HTTP GET request to the IP address "10.0.0.1."
  *   The behavior of this command depends on whether there is a web server running on that IP address and whether the web server is configured to respond to HTTP requests.
  *   Here are the possible outcomes:
    *   If there is a Web Server: If there is a web server running on the IP address "10.0.0.1" and it is configured to respond to HTTP requests on the default HTTP port (port 80), Curl will make a GET request to the root directory of the web server (i.e., http://10.0.0.1/). If the web server responds, Curl will display the content of the web page in the terminal.
    *   If there is no Web Server: If there is no web server running on the specified IP address or the web server is not configured to respond to HTTP requests, Curl will typically display an error message indicating that it could not connect to the host or that the connection was refused.

Curl is a command-line tool and library for transferring data with URLs. While it is not a dedicated reconnaissance tool, it can be used for various reconnaissance tasks, especially when interacting with web servers and APIs. Here are some common reconnaissance tasks you can perform with Curl:

1. **HTTP Banner Grabbing**:
   - To retrieve HTTP headers and identify the web server software, you can use Curl to make a simple HTTP GET request to a target web server:

     ```bash
     curl -I http://example.com
     ```

2. **Web Directory Enumeration**:
   - You can use Curl to retrieve a list of files and directories from a web server by sending an HTTP GET request to the root directory or specific paths:

     ```bash
     curl -s http://example.com/
     curl -s http://example.com/admin/
     ```

3. **HTTP Method Testing**:
   - To test which HTTP methods are supported by a web server, including methods like PUT and DELETE, you can use Curl:

     ```bash
     curl -X OPTIONS http://example.com
     curl -X PUT http://example.com/testfile.txt
     ```

4. **Testing for Vulnerabilities**:
   - Curl can be used to send different types of requests to test for vulnerabilities, such as SQL injection or XSS (Cross-Site Scripting). For example:

     ```bash
     curl -d "user=admin&password=' OR 1=1 --" http://example.com/login
     ```

5. **API Testing**:
   - If you're testing an API, you can use Curl to send HTTP requests with specific headers, payloads, and authentication:

     ```bash
     curl -H "Authorization: Bearer YOUR_API_TOKEN" https://api.example.com/resource
     ```

6. **Response Analysis**:
   - You can use Curl to retrieve and save HTTP responses to a file for later analysis:

     ```bash
     curl -o output.html http://example.com
     ```

Remember that while Curl can be a handy tool for reconnaissance, it's crucial to use it responsibly and within the scope of authorized testing or research. Unauthorized or aggressive use of Curl against a target without permission may be considered unethical and potentially illegal. Always obtain proper authorization before conducting any reconnaissance activities on systems that you do not own or have explicit permission to test.

***
* Another way to retrieve web pages: `wget [IP]`
* `browsh --startup-url [IP]`  --> It will render the website in the command line.
* 'lynx [IP]` ---> similar to browsh
* `dirb [URL] [optional wordlist]`
  * where Wordlists are crucial for the effectiveness of dirb.
  * They contain a list of common directory and file names that dirb will use to guess and check if they exist on the target website.
  * You can create your own wordlist or use existing ones designed for web directory brute-forcing.
 
* **robots.txt**
  The `auxiliary/scanner/http/robots_txt` module in Metasploit is designed for scanning and gathering information from the `robots.txt` file of a target web server during a penetration test or security assessment. The `robots.txt` file is a standard used by websites to communicate with web crawlers and search engines, specifying which parts of the site should be crawled and which should not. Here's how the `robots_txt` module works in Metasploit:

1. **Loading the Module**:
   - You can load the `robots_txt` module in Metasploit by running the `use` command followed by the module's full name:

     ```
     use auxiliary/scanner/http/robots_txt
     ```

2. **Setting Options**:
   - After selecting the module, you need to set various options to configure the scan. You can use the `show options` command to view and set these options. Some of the key options you can configure include:
     - `RHOSTS`: This option specifies the target host or range of hosts to scan. You can specify a single IP address or a range.
     - `RPORT`: This is the port to use for the HTTP requests, typically port 80 for HTTP.
     - `THREADS`: The number of concurrent threads to use for scanning.
     - `FILEPATH`: The path to the `robots.txt` file. By default, it's set to `/robots.txt`, but you can change it if needed.

3. **Running the Scan**:
   - Once you've configured the necessary options, you can initiate the scan by running the `run` command. Metasploit will send HTTP requests to the target(s) and retrieve the `robots.txt` file if it exists.

4. **Scan Results**:
   - The `robots_txt` module will display the results of the scan in the Metasploit console. It will show the content of the `robots.txt` file, which may include directives like "Disallow" and "Allow" specifying which parts of the website are accessible to web crawlers.

Here's an example of using the `robots_txt` module in Metasploit:

```plaintext
msf6 auxiliary(scanner/http/robots_txt) > set RHOSTS 10.0.0.1
RHOSTS => 10.0.0.1
msf6 auxiliary(scanner/http/robots_txt) > run
[*] Scanning http://10.0.0.1/robots.txt
[*] Robots.txt for http://10.0.0.1/robots.txt
[*]  User-agent: *
[*]  Disallow: /private/
[*]  Allow: /public/
```

In this example, the module scans the `robots.txt` file on the target host at `http://10.0.0.1/robots.txt` and displays the contents of the file, showing the "Disallow" and "Allow" directives.

The `robots_txt` module can be useful for understanding a website's intended crawling behavior and identifying restricted or sensitive areas of a web application that should not be crawled by web spiders and search engines. 
